---
title: "Predicting Car Crashes Caused by Alcohol Using Logistic Regression"
author: "Li, Jie; Wang, Yan; Zhang, Yihan"
date: "11/2o/2022"
output:
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: hide
    theme: journal
    fig_caption: yes
  pdf_document: default
monofont: Hack
editor_options: 
  markdown: 
    wrap: sentence
---

<link rel="stylesheet" href="css/styles.css">

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=FALSE, results="hide", message=FALSE, warning=FALSE)
```

```{r set-up}

# Several libraries and util functions from my GitHub
source("https://raw.githubusercontent.com/Leejere/r-setup/main/r_setup.r")
library(gmodels)
library(ROCR)
library(plotROC)
```

# Introduction

Alcohol is a major factor related to vehicle accidents. According to a study conducted by the National Highway Traffic Safety Administration, the economic impact of alcohol-related crashes exceeds 59 billion dollars annually. In this assignment, we aimed to identify some of the factors contributing to alcohol-related crashes in the city of Philadelphia. More specifically, we used logistic regression to explore what factors make vehicle crashes more likely caused by drinking.

Nine predictors were entered that we speculated might be associated with alcohol involvement in car accidents. Seven predictors are attributes of the crashes themselves, and two more the demographic attributes of the census block group (CBG) where the accident took place.

-   Involvement of fatality or major injury (`fatal_injury`) and involvement of overturned vehicle(s) (`overturned`). It is likely that highly fatal or serious accidents are likely to be associated with alcohol.

-   Involvement of phone usage (`phone`), speeding (`speeding`), and aggressive driving (`aggressive`). These other highly risky driving behaviors might also cause crashes on their own independent from drinking. These behaviors may also be associated with the behavior of drinking.

-   Involvement of senior drivers 65 years or older (`senior_driver`) or teenage drivers 16 to 17 years old (`teen_driver`). It is likely that different age groups differ in the primary causes for car accidents they are involved in.

-   Demographic attributes of the CBG where the accident took place, i.e., percentage of residents holding a bachelor's degree or higher (`pct_bachelor`) and median household income (`md_hh_icome`). It is plausible that accident causes differ among different demographic groups.

We used R for all the analysis, regression, and data visualization in this assignment.

# Methods

## The logistic model

In the OLS regression performed in the previous assignments, we dealt with dependent variables whose values were *continuous* with a reasonably *large range*. In this way, the models could be explained as one unit's change of a predictor correlating with certain units' change of the dependent variable, holding other variables constant. However, this explanation no longer applies when the dependent variable is binary, as it only takes values of either 0 or 1. We need a different approach.

The logistic regression method works around this problem by turning the $y$ in the equation from *binary* to continuous *probability*. In this way, we may say that one unit's change of a predictor is correlated with certain increase or decrease in the probability of the dependent variable being "True".

However, this brings another problem, as probability only ranges from 0 to 1, whereas in a linear model, the $y$ should have no lower and upper bounds. To solve this problem, we can use a “translator” function to construct a new variable, $ln\frac{p}{1-p}$, in which $p$ is the probability of the dependent variable being "True" ($p=P(drink=1)$). The ratio of "True" probability against "False" probability is called the *odds*. By wrapping the *odds* inside of a log-transformation, we have successfully turned the scope of $y$ to from $-\infty$ to $\infty$.

This translator function of $ln(\frac{p}{1-p})$ is called the the *logit function*. As the input probability approaches zero, the output logit odds approaches $-\infty$; conversely, if the input probability is close to 1, the output logit odds reaches $\infty$. In this way, the binary variable is successfully transformed into a continuous one with an infinite scope, so that we can use it as the new $y$ and fit a linear model.

In our case, the final equation for the logistic model is written as follows:

$$
ln(\frac{p}{1-p})=\beta_0 + \beta_1\cdot fatal\_injury + \beta_2\cdot overturned + \beta_3\cdot phone + \\beta_4\cdot speeding + \beta_5\cdot aggressive + \beta_6\cdot teen\_driver + \\beta_7\cdot senior\_driver + \beta_8\cdot pct\_bachelor + \beta_9\cdot md\_hh\_income + \epsilon
$$

where $ln(\frac{p}{1-p})$ is the "log odds" where $p$ is the probability of the crash being caused by alcohol, $\beta_0$ is the intercept coefficient, $\beta_1$ to $\beta_n$ are the coefficients of the nine predictors, and $\epsilon$ is the residual. The model is estimated through the Maximum Likelihood Estimation method.

The significance of each coefficient is tested through hypothesis testing. For each coefficient $\beta_i$, the *null hypothesis* ($H_0$) is that $\beta_i=0$. As $\beta_i$ has a normal distribution with a standard deviation of $\sigma_{\beta_i}$, we can calculate a Z-score through $\frac{\hat\beta_i - 0}{\sigma_{\beta_i}}$ where $\hat\beta_i$ is the coefficient estimated from the model. If the p-value associated with the Z-score goes under 0.05, we reject the null hypothesis for the *alternative hypothesis* that $\beta_i\neq0$.

To interpret the coefficients, most statisticians use a concept of *odds ratio*, which is calculated by exponentiating the $\beta$ coefficients ($e^{\beta_i}$). The odds ratio is interpreted as follows: for every unit of change in the predictor $x_i$, the odds of the dependent variable being “True” against “False” increases (or shrinks) by a factor of $e^{\beta_i}$. 

It should be noted that the logistic model only outputs the estimated *probabilities*; and to obtain the predictions for the dependent variable, we need to determine a “cutoff probability”, above which the dependent variable is predicted to be true and below which false.

## Goodness of fit

How do we determine the goodness of fit of a logistic regression model? As in the OLS model, $R^2$ can be estimated, but it no longer bears the interpretation as the ratio of variance explained by the model. To compare the goodness of fit of multiple nested models, we may use the *Akaike Information Criterion*, or AIC, a smaller value of which indicates a better fit compared to the baseline model.

As mentioned in the previous section, the predictions for the dependent variable depend on the "cutoff probability" that we set. An observation is predicted to be true ($\hat y=1$), if the estimated probability $p$ is higher than the threshold, and vice versa. Then, each observation is categorized into one of the four following categories in Table 1.:

```{r category-table, echo=FALSE, results="asis"}

table = data.frame("Observed True" = c("TP: True Positive", "FN: False Negative"),
                   "Observed False" = c("FP: False Positive", "TN: True Negative"))
colnames(table) = c("Observed True", "Observed False")
rownames(table) = c("Predicted Positive", "Predicted Negative")

table %>% 
  kable(caption = "Table 1. Four outcomes of a logistic model") %>%
  kable_classic(full_width = FALSE, html_font = "Cambria")
```

<br />

The goal is to find the best cutoff value that maximizes TP and TN, while minimizing FP and FN. Specifically,
-   *Sensitivity*, or *True Positive Rate*, stands for how well the model is able to pick out the True observations, and is calculated as $\frac{TP}{TP+FN}$.

-   *Specificity*, or *True Negative Rate* stands for how well the model accurately predicts false values, and is calculated as $\frac{TN}{TN+FP}$.

Deciding on the right cutoff value is often a trade-off process. If the cutoff value is set to be relatively high, then fewer observations will be predicted as true. This sacrifices sensitivity, but the specificity increases. On the flip side, a lower cutoff value categorizes more observations to be true, often increasing sensitivity but lowering specificity.

This trade-off process can be visualized using the *ROC curve*, in which the x-axis stands for false positive rate, or one minus specificity, and the y-axis stands for true positive rate or sensitivity. As you lower the cutoff value, the false positive rate goes up, whereas the true positive rate also goes up, forming a curve depicted as follows:

<img src="./pic/roc-curve-diagram.png" alt="roc-diagram" width="300"/>
<p class="caption">Fig. 1. Diagram of an ROC curve</p>

This diagram can be used as a guide to spot the best combination of reasonably high sensitivity and specificity, i.e., a point as close to the upper-left corner as possible. A few methods exist the determine this point: 

-   By maximizing the sum of sensitivity and specificity, or the *Youden Index*.

-   By minimizing the distance on the ROC plot from the curve to the upper left corner.

If we have a good model, all the points on the ROC curve (no matter the cutoff value) will be generally closer to the upper-left corner, and the area under the curve will be greater. Therefore, the *Area Under the Curve*, or AUC, is another criterion of the goodness of fit of the logistic model. The better the model’s fit, the closer the AUC is to 1. AUC may be interpreted as the probability that, given two random cases, the observed “True” will be predicted a higher probability than the observed “False”.

## Model assumptions

Like the OLS model, the logistic model also has a number of assumptions required of the data. These assumptions are:

-   The dependent variable is binary.

-   The observations are independent from each other.

-   There is no severe multi-collinearity among the predictors.

-   Large samples size is required, as the model is estimated through the Maximum Likelihood Estimation method.

Unlike the OLS, however, the logistic regression model makes no assumption that the relationship between the dependent variable and each predictor is normal. It does not require homoscedasticity, nor the normality of the distribution of residuals.

## Pre-regression explorations

Before running a logistic regression, statisticians often carry out a series of exploratory analysis.

**Cross-tabulation between the dependent variable and categorical predictors**. We can cross-tabulate all the observations into groups, each group being a combination of the true/false value of the dependent and a class of the predictor. Using this tabulation, a *chi-square test* is performed. 

The null hypothesis of a chi-square test is that there is no significant difference in terms of outcome (dependent variable) between the classes, and therefore cases should be *randomly distributed* across the tabulated categories; as opposed to the alternative hypothesis of there being a significant difference. $\chi^2$ is estimated for this tabulation:

$$
\chi_2 = \sum\frac{(O_i-E_i)^2}{E_i}
$$

where $O_i$ is the observed frequency of each category, and $E_i$ is the expected frequency of each category in random distribution. If the null hypothesis stands, then the chi-square value should be of chi-square distribution. If the p-value of the observed chi-square is reasonably small (i.e., smaller than 0.05), then we may reject the null hypothesis for the alternative hypothesis and state that the predictor is indeed correlated with the dependent variable.

**Independent-samples T-tests**. To examine the correlation between dependent variable and a continuous predictor, we may calculate the mean value of the predictor by the binary outcome. The true or false of the binary outcome essentially divides all the observations into two samples, and then we can test whether there is a significant difference between the two samples through an independent-samples T-test. 

The null hypothesis for the independent-samples T-test is that there is no significant difference between the two samples; and therefore there should be no significant difference between the sample means. If such hypothesis stands, then the difference of the two sample means will be of T-distribution with a mean of zero. If the observed T-score is associated with a p-value lower than 0.05, then we reject the null hypothesis for the alternative hypothesis that there is significant difference between the two samples, and therefore the predictor is correlated with the dependent variable.

# Data exploration

Before performing the logistic regressions, we first explored the relationship between the predictors and the dependent variable, and between the predictors themselves. The below code chunk imports the data and sets up the variable names for the imported data frame.

```{r import}

# Import data
df = read.csv("data/Logistic Regression Data.csv") %>%
  dplyr::select(record_id = CRN, # Crash record number
                drink = DRINKING_D, # Drinking driver indicator (boolean)
                fatal_injury = FATAL_OR_M, # Crash resulted in fatality or major injury (boolean)
                overturned = OVERTURNED, # Crash involved an overturned vehicle (boolean)
                phone = CELL_PHONE, # Driver was using a cell phone (boolean)
                speeding = SPEEDING, # Crash involved speeding car (boolean)
                aggressive = AGGRESSIVE, # Crash involved aggressive driving (boolean)
                teen_driver = DRIVER1617, # Car involved at least one driver who was 16 or 17 years old (boolean)
                senior_driver = DRIVER65PLUS, # Crash involved at least one driver was was at least 65 years old (boolean)
                pct_bachelor = PCTBACHMOR, # Pct residents in CBG with bachelor's degree or higher
                md_hh_income = MEDHHINC, # Median household income of CBG
                )

# Dictionary of var names to their real names
names_dict = 
  c("record_id" = "Record ID",
    "drink" = "Involved drnking driver",
    "phone" = "Involved phone usage",
    "fatal_injury" = "Involved fatality or major injury",
    "overturned" = "Involved overturned vehicle",
    "speeding" = "Involved speeding car",
    "aggressive" = "Involved aggressive driving",
    "teen_driver" = "Involved driver 16 to 17 years old",
    "senior_driver" = "Involved driver 65 years or older",
    "pct_bachelor" = "Pct residents in CBG w/ at least bachelor's degree",
    "md_hh_income" = "Median houshold income of CBG")

```

## The dependent variable

Let's first take a quick look at the dependent variable through Table 2.
5.7% of all the crashes in the data set involved a drinking driver and therefore possibly caused by drinking.
The other 94.3% of the crashes did not involve a drinking driver.

```{r dv-tab, results="asis"}

# Tabulation of the dependent variable
table(df$drink) %>%
  as.data.frame() %>%
  mutate("Drinking Status" = ifelse(Var1 == 1, "Involved drinking driver",
                                    "Did not involve drinking driver")) %>%
  dplyr::select("Drinking Status", Freq) %>%
  mutate(Proportion = (Freq * 100 / sum(Freq)) %>% round(1) %>% paste0("%")) %>%
  kable(caption = "Table 2. Tabulation of the dependent variable") %>%
  kable_classic(full_width=FALSE, html_font="Cambria")
```
<br />

## The binary predictors

In this section, we explored the relationship between the dependent variable and the 7 *binary* predictors:

-   `fatal_injury`: involved fatality or major injuries.
-   `overturned`: involved overturned vehicle.
-   `phone`: involved cell-phone usage.
-   `speeding`: involved speeding vehicle.
-   `aggressive`: involved aggressive driving.
-   `teen_driver`: involved driver of 16 to 17 years old.
-   `senior_driver`: involved driver 65 years or older.

The below two code chunks performs a cross-tabulation between the dependent variable and the seven binary predictors, as shown in Table 3. The percentages indicate what percentages of crashes (either involving a drinking driver or not) are "true" in their respective predictors. The last column is the p-value corresponding to the $\chi^2$ test of the dependent variable and each predictors.

```{r cross-tabs, message=FALSE}

# List of variables for this tabulation
var_list = c("fatal_injury", "overturned", "phone", "speeding", "aggressive",
             "teen_driver", "senior_driver")

# Sum all counts and percentages by drinking/nondrinking status, and transpose result
sum_table = df %>%
  dplyr::select(drink, all_of(var_list)) %>%
  group_by(drink) %>%
  summarise_all(c(sum=sum, avg=mean)) %>% # sum means total count, avg means percentage of of positive values
  t() %>%
  as.data.frame()

# Get rid of first row, dependent variable
sum_table = sum_table[-1,] 

# Organize the table into the desirable output
sum_table = sum_table %>%
  mutate(index = rownames(sum_table),
         var_name = index %>% substring(1, nchar(index) - 4),
         func_name = index %>% substring(nchar(index) - 2, nchar(index))) %>%
  rename(no_alcohol = V1, alcohol = V2) %>%
  dplyr::select(no_alcohol, alcohol, var_name, func_name)

sum_table_sum = sum_table %>% filter(func_name == "sum") %>%
  dplyr::select(no_alcohol, alcohol, var_name) %>%
  rename(count_no_alcohol = no_alcohol, count_alcohol = alcohol)
sum_table_avg = sum_table %>% filter(func_name == "avg") %>%
  dplyr::select(no_alcohol, alcohol, var_name) %>%
  mutate(no_alcohol = (no_alcohol * 100) %>% round(1) %>% paste0("%"),
         alcohol = (alcohol * 100) %>% round(1) %>% paste0("%")) %>%
  rename(pct_no_alcohol = no_alcohol, 
         pct_alcohol = alcohol)

output = sum_table_sum %>%
  left_join(sum_table_avg, on = "var_name") %>%
  dplyr::select(var_name, count_no_alcohol, pct_no_alcohol, count_alcohol, pct_alcohol)

# Organize final output suitable for kable
rownames(output) = names_dict[output$var_name] %>% unname()
output = output %>% dplyr::select(-var_name)

```

```{r chi-squares, results="asis"}

# Calculate the chi-squares for each binary predictor with the dependent variable

# Initialize a vector of chi-squares
chisq_list = c()

for(var in var_list) {
  this_test = chisq.test(df[[var]], df$drink)
  this_p = this_test$p.value %>% round(3)
  chisq_list = c(chisq_list, this_p)
}

# Add result to output table
output = output %>% cbind(chisq_list) %>%
  rename("Chi-Square p-value" = chisq_list)

# Decorate output table
colnames(output) = c("N", "%", "N", "%", "Chi-square p-value")
output %>%
  kable(caption = "Table 3. Cross tabulations of the dependent variable and binary predictors") %>%
  add_header_above(c(" ", "No alcohol involved" = 2, "Alcohol involved" = 2, "Chi-square test" = 1)) %>%
  kable_classic(full_width=FALSE, html_font="Cambria")
```

<br />
The chi-square test results show that apart from cell-phone usage, all the other binary predictors are significant correlated with the dependent variable.

## The continuous predictors

Next, we explored the dependent variable and the two continuous predictors:

-   `pct_bachelor`: percentage of residents with a bachelor's degree or higher in the census block group (CBG) where the crash took place.

-   `md_hh_income`: median household income of the census block group (CBG) where the crash took place.

The below two code chunks perform a tabulation between the dependent variable and the two predictors (Table 4.), summarizing the mean and standard deviation of each predictor, categorized by the dependent variable (involved a drinking driver or not).
The last column shows the p-value from the independent-samples T-Test of each predictor by the dependent variable.


```{r tab-continuous, message=FALSE}

# List of variables used in this calculation
var_list_2 = c("pct_bachelor", "md_hh_income")

# Summarize the results we need, and transpose the result
sum_table_2 = df %>%
  # Normalize income figures by thousand
  mutate(md_hh_income = md_hh_income / 1000) %>%
  dplyr::select(drink, all_of(var_list_2)) %>%
  group_by(drink) %>%
  summarise_all(c(avg=mean, std=sd)) %>% # sum means total count, avg means percentage of of positive values
  t() %>%
  as.data.frame()

# Get rid of first row, dependent variable
sum_table_2 = sum_table_2[-1,]

# Organize the table
sum_table_2 = sum_table_2 %>%
  mutate(index = rownames(sum_table_2),
         var_name = index %>% substring(1, nchar(index) - 4),
         func_name = index %>% substring(nchar(index) - 2, nchar(index))) %>%
  rename(no_alcohol = V1, alcohol = V2) %>%
  dplyr::select(no_alcohol, alcohol, var_name, func_name)

sum_table_std = sum_table_2 %>% filter(func_name == "std") %>%
  dplyr::select(no_alcohol, alcohol, var_name) %>%
  mutate(no_alcohol = no_alcohol %>% round(2),
         alcohol = alcohol %>% round(2)) %>%
  rename(count_no_alcohol = no_alcohol, count_alcohol = alcohol)

sum_table_avg = sum_table_2 %>% filter(func_name == "avg") %>%
  dplyr::select(no_alcohol, alcohol, var_name) %>%
  mutate(no_alcohol = no_alcohol %>% round(2),
         alcohol = alcohol %>% round(2)) %>%
  rename(pct_no_alcohol = no_alcohol, 
         pct_alcohol = alcohol)

output_2 = sum_table_avg%>%
  left_join(sum_table_std, on = "var_name") %>%
  dplyr::select(var_name, count_no_alcohol, pct_no_alcohol, count_alcohol, pct_alcohol)

# Organize final output suitable for kable
rownames(output_2) = names_dict[output_2$var_name] %>% unname()
output_2 = output_2 %>% dplyr::select(-var_name)
```

```{r ind-sample-ttest, results="asis"}

# Initialize of vector to store the independent-samples t-test results
ttest_list = c()

for(var in var_list_2) {
  this_test = t.test(df[[var]] ~ df$drink)
  this_p = this_test$p.value %>% round(3)
  ttest_list = c(ttest_list, this_p)
}

output_2 = output_2 %>% cbind(ttest_list) %>%
  rename("T-Test p-value" = ttest_list)

colnames(output_2) = c("Mean", "SD", "Mean", "SD", "T-Test p-value")

output_2 %>%
  kable(caption = "Table 4. Cross tabulations of the dependent variable and continuous predictors") %>%
  add_header_above(c(" ", "No alcohol involved" = 2, "Alcohol involved" = 2, "Independent Samples T-Test" = 1)) %>%
  kable_classic(full_width=FALSE, html_font="Cambria")
```

<br />
The table shows that both independent-samples T-tests produced a p-value greater than 0.05, meaning that the two continuous variables themselves are not significantly correlated with the dependent variable.

## Checking for multi-collinearity among the predictors

This section tests whether and how the predictors are inter-correlated with each other, although logistic regressions generally do not require for non-collinearity. Table 5. shows the correlation coefficient ($r$) between each pair of predictors, and Table 6.
presents the p-values.

```{r check-collin, results="asis"}

# Calculate the correlation between each pair of predictors
rcorr = 
  df %>% 
  dplyr::select(-record_id, -drink) %>% 
  as.matrix() %>% 
  rcorr(type = "pearson")

# Rounding numbers for better presentation
r_matrix = rcorr$r %>% round(2)
p_matrix = rcorr$P %>% round(2)

p_matrix[is.na(p_matrix)] = "/"

# Use formal variable names in the table
var_names = names_dict[colnames(r_matrix)] %>% unname()
colnames(r_matrix) = var_names
colnames(p_matrix) = var_names
rownames(r_matrix) = var_names
rownames(p_matrix) = var_names

r_matrix %>%
  kable(caption = "Table 5. Correlation matrix of predictors") %>%
  kable_classic(html_font="Cambria")

```

```{r check-collin-p, results="asis"}

p_matrix %>%
  kable(caption = "Table 6. Correlation matrix (p-values) of predictors") %>%
  kable_classic(html_font="Cambria")
```

<br />
The tables above shows that there is some collinearity between some predictors. However, the correlations among the predictors are very weak.

# Logistic Regression Results

## The base model

Having performed the exploratory analysis, this section focuses on the logistic regression itself. First, we ran a logistic regression (base) that includes all the nine predictors. The model summary is presented below.

```{r reg-all-predictors, results="asis"}

# Logistic Regression: All Nine Predictors
reg_base <- glm(drink ~ 
                  fatal_injury + 
                  overturned + 
                  phone + 
                  speeding + 
                  aggressive + 
                  senior_driver + 
                  teen_driver + 
                  pct_bachelor + 
                  md_hh_income, 
                data=df, family = "binomial" (link = "logit"))

base_summary = summary(reg_base)
aic = (summary(reg_base))$aic %>% round(2)

# Outputting the coefficient table
base_coefficient = base_summary$coefficients %>% as.data.frame()

# Util function to make a nice-looking coefficient table output
make_coef_table = function(coefficient_df, caption) {
  # Decorating the output table
  table_row_names = names_dict[rownames(coefficient_df)] %>% unname()
  table_row_names[is.na(table_row_names)] = "(Intercept)"
  rownames(coefficient_df) = table_row_names
  
  coefficient_df = coefficient_df %>%
    # Calculate the odd ratio
    mutate("Odd Ratio" = exp(Estimate) %>% round(4),
           "Beta" = Estimate %>% round(2),
           p_value = coefficient_df[["Pr(>|z|)"]] %>% round(3),
           p_value = case_when(p_value < 0.001 ~ sprintf("%.3f", p_value) %>% paste0("***"),
                               p_value < 0.01 ~ sprintf("%.3f", p_value) %>% paste0("**"),
                               p_value < 0.05 ~ sprintf("%.3f", p_value) %>% paste0("*"),
                               TRUE ~ sprintf("%.3f", p_value) %>% paste0(""))) %>%
    dplyr::select("Beta", "Odd Ratio", "P-Value" = p_value)
  
  coefficient_df %>%
    kable(caption = caption) %>%
    kable_classic(full_width = FALSE, html_font = "Cambria")
}

make_coef_table(base_coefficient, 
                "Table 7. Coefficient estimates for the base model (AIC = " %>% 
                  paste0(aic) %>% paste0(")"))

```

<br />
As shown on Table 7., the model produced an overall AIC of 18,359.69. Seven out of the nine predictors turned out significant (with p-values smaller than 0.05). Two variables, namely phone usage and the percentage of residents with a bachelor's degree or higher in the CBG, were not significant. Let us explain the significant model coefficients through *odd ratios*:

-   The odd ratio for the predictor “Involved fatality or major injury” is 2.26, meaning that compared to accidents with no fatality nor major injury, if the accidents were with fatality or major injury, then the odds of the crash being associated with drinking goes up by 126% (2.26 minus 1).

-   Likewise, if the crash involved any overturned vehicle, then the odds of it being associated with drinking goes up by 153%.

-   If the crash involved speeding, then the odds of it being associated with drinking goes up by 366%.

-   If the crash involved aggressive driving, then the odds of it being associated with drinking is lowered by a factor of 0.55, or 45% lower. 

-   If the crash involved a senior driver of 65 years or older, then the odds of it being associated with drinking is lowered by a factor of 0.46, or 54% lower. 

-   If the crash involved a teen driver of 16 or 17 years old, then the odds of it being associated with drinking is lowered by a factor of 0.28, or 72% lower. 

-   Although the coefficient for median household income is significant, it is almost negligibly small. As the median household income of the CBG where the crash took place goes up by 1,000 dollars, the odds of it being associated with drinking barely changes.

## Determining the cut-off value

```{r sens_spec_cut_off_table, results="hide"}

# Construct a dataframe of the observed value and predicted probabilities
fit_df <- data.frame(observed = df$drink, fitted = reg_base$fitted.values)

# A list of cutoff thresholds
cut_off <-  c(0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.5)

# Initialize a table to store the sensitivity, specificity, and misclassfication rates of each cutoff
cut_off_df <- data.frame(Cut_Off_Value = double(),
                         Sensitivity = double(),
                         Specificity = double (),
                         Misclassification_Rate = double())

# Populate the data frame initialized above
for (var in cut_off) {
  # Make predictions using this cutoff value
  this_df <-  fit_df %>% 
  mutate(fit_binary = ifelse((fitted > var), 1, 0))
  
  cross_table  <- CrossTable(this_df$fit_binary, this_df$observed, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE)
  
  cut_off_df <- cut_off_df %>% add_row(
    Cut_Off_Value = var,
    Sensitivity = round(cross_table$prop.col[2,2], digits = 3),
    Specificity = round(cross_table$prop.col[1,1], digits = 3),
    Misclassification_Rate = round(
      (cross_table$t[1,2] + cross_table$t[2,1])/(cross_table$t[1,1] + cross_table$t[1,2] + cross_table$t[2,1] +cross_table$t[2,2]),
      digits = 3
    )
  )
}

colnames(cut_off_df) = c("Cut-off value", "Sensitivity", "Specificity", "Misclassification rate")

```


How do we determine the cutoffs? In Fig. 2., we plotted the probability density of the predicted probabilities by observed outcome. We found that for most cases, the model predicted a very low probability for drinking involvement, and that the distinction between two observed categories were not clear-cut. 

```{r prep-for-cutoff-viz, fig.cap="Fig. 2. Distribution of predicted probabilities by observed outcome (base model)", fig.align="center"}

# Distribution of predicted probabilities by observed outcome

fit_df %>%
  mutate(observed = ifelse(observed == 1, "Observed to involve alcohol",
                           "Observed to involve no alcohol")) %>%
  ggplot() +
  geom_density(aes(x = fitted, fill = as.factor(observed)), color = NA) +
  facet_grid(observed ~ .) +
  scale_fill_manual(values = c(palette_hero_faded, palette_primary)) +
  geom_vline(xintercept = 0.063, color = palette_hero, size = 0.8, linetype = "dashed") +
  xlim(0, 1) +
  labs(x = "Predicted probabilities of outcome", y = "Distribution",
       title = "Distribution of predicted probabilities by observed outcome") +
  plot_theme() +
  theme(legend.position = "none")

```
On Table 8., we calculated the sensitivity, specificity, and misclassification rate corresponding to a series of cut-off values. As the cut-off values increase, the misclassification rate goes continuously down. In the table, the cut-off value associated with the highest misclassification rate is 0.02, and the lowest 0.50. 

However, this does not mean that we should aim for the lowest misclassification rate possible. Recall that the observed “True” cases only make up about 5.7% of all the data (see Table 1.), and as one raise the cut-off value, the misclassification rate gradually approximates this percentage. If you predict *all* cases to be “False”, then you get a misclassification rate of 0.057, which is lowest on Table 5, but such predictions also become worthless. Therefore, we need to consider the combination of sensitivity and specificity to determine the best cut-off value.


```{r cut_off_table_present, results="asis"}

cut_off_df %>%
  kable(caption = "Table 8. Sensitivities, specificities, and misclassification rates corresponding to a series of cutoffs") %>%
  kable_classic(full_width=FALSE, html_font="Cambria")

```

<br />
In Fig. 3., we plotted the ROC curve for the base model. The curve spans above the 45-degree line, meaning that the model is not totally worthless. The AUC of the curve is 0.6399, meaning that, given two random cases, the observed “True” will have a 64% chance of being predicted a higher probability than the observed “False”.

```{r roc-curve, fig.cap="Fig. 2. ROC curve and AUC of the base model", fig.align="center"}

library(plotROC)

# plot ROC, calculate AUC
ggplot(fit_df, aes(d = observed, m = fitted)) +
  geom_roc(n.cuts = 100, labels = FALSE, colour = palette_hero) +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve",
       subtitle = paste("Area Under Curve (AUC): ", 
                        round(pROC::auc(fit_df$observed, fit_df$fitted), digits = 4)))+
  plot_theme()

```

Based on the ROC curve, we then tried to locate the best cut-off value by minimizing the euclidean distance from the point to upper left corner (Point $(0, 1)$).

```{r optimal_cut_off}

library(ROCR)

# Create a prediction instance
predictions <- prediction(predictions = fit_df$fitted, labels = fit_df$observed)


# Outputs ROC performance object
roc_performance = performance(predictions, measure = "tpr", x.measure="fpr")

locate_optimal_cutoff = function(
    x, # vector of x values (FP rates)
    y, # vector of y values(TP rates)
    p # cutoff values corresponding to x and y
  ) {
  # A list of squared distances
  dist_sq = (x - 0) ^ 2 + (y - 1) ^ 2
  
  # Location of the x/y where distance is minimized
  ind = which(dist_sq == min(dist_sq))
  
  # Return the sensitivity, specificity, and cutoff of the optimal point
  result = c(sensitivity = y[[ind]], specificity = 1-x[[ind]],
             cutoff = p[[ind]])
  
  return(result)
}

# Function to calculate the distance to the upper left corner from each point
# on the ROC curve (on of the 8,000+ points)

find_optimal_cutoff = function(performance, predictions){
  cut.ind = mapply(locate_optimal_cutoff, 
                   performance@x.values, 
                   performance@y.values, 
                   predictions@cutoffs)
  return(cut.ind)
}
# This will print the optimal cut-off point and the corresponding
# specificity and sensitivity
print(find_optimal_cutoff(roc_performance, predictions))

```

The model output shows that 0.637 is the cut-off value that minimizes the distance from the curve to the point $(0, 1)$. With this cut-off value, the model yielded a sensitivity of 0.66 and a specificity of 0.55.

## The second model

In this section, we tested a second model that included only the seven binary predictors, excluding median household income and the percentage of residents with a bachelor's degree or higher.

```{r reg-binary-predictors, results="asis"}

# Logistic Regression: only the seven binary predictors
reg_bi_only <- glm(drink ~ 
                     fatal_injury + 
                     overturned + 
                     phone + 
                     speeding + 
                     aggressive + 
                     senior_driver + 
                     teen_driver, 
                   data=df, 
                   family = "binomial" (link = "logit"))

bi_only_summary = summary(reg_bi_only)
aic = bi_only_summary$aic %>% round(2)

# Outputting the coefficient table
bi_only_coefficients = bi_only_summary$coefficients %>% as.data.frame()

# Output kable
make_coef_table(bi_only_coefficients,
                "Table 9. Coefficient estimates for the second model (AIC = " %>% 
                  paste0(aic) %>% paste0(")"))

```

<br />
Table 9. summarizes the model outputs. All the predictors that were significant in the base model were still significant in this model, and the predictor of phone usage remained insignificant. The AIC for this model is 18,360, which is almost the same as the base model, meaning that the two models are fairly equal in their goodness of fit.

# Discussion

In this assignment, we ran two logistic regression models to explore what factors make vehicle crashes more likely caused by drinking. We entered nine predictors into the models, in which seven are binary, and two are continuous. The model results show that the following predictors are positively related to alcohol-caused crashes:

-   Involvement of fatality or major injury;

-   Involvement of any overturned vehicle;

-   Involvement of speeding.

On the flip side, the following predictors are negatively related to alcohol-caused crashes:

-   Involvement of aggressive driving;

-   Involvement of senior drivers 65 years or older;

-   Involvement of teenage drivers 16 to 17 years old.

Phone usage and the percentage of bachelor's (or higher) degree holders did not turn out to be significant predictors. Median household income was a significant predictor, but its coefficient was so low and even negligible.

The model coefficients were not very surprising. For example, the model showed that serious accidents — fatal and injury-inducing crashes or crashes with overturned vehicles — were more likely to be related to alcohol. The model also suggested that accidents from speeding were more likely caused by alcohol. On the other hand, crashes that involved aggressive driving were negatively associated with alcohol, probably because crashes are often caused by aggressive behavior unrelated to alcohol. Regarding drivers’ age, the model suggested that teenage and senior drivers are less likely involved in accidents caused by drinking, but rather other accidents of other causes.

Overall, our regression models did not perform very well, with an AUC of only 65%. We would not say that the logistic regression was inappropriate for our purpose: although the percentage of “Trues” for the dependent variable is small, the aggregate number of “True” cases was 2,485, which was a decent amount for the Maximum Likelihood Estimation method. 

Rather, it could our *predictors* that posed the greatest limitation, as they do not possess enough explanation power. With ideal predictors, the predicted probabilities should differentiate between by the observed categories, namely, the observed “True” cases should generally be predicted to have high probabilities, and the observed “False” cases should be predicted to have low probabilities. In our case, however, Fig. 2. showed that the predicted probabilities hardly differentiate by the two observed categories. Therefore, we could not reach high sensitivity and specificity no matter what cut-off value we selected. It is likely that the methods to predict rare events as suggested by Paul Allison might not help in our case, if no better predictors are available.

